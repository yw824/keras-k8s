{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 강화학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지도학습에서는, 지도가 존재한다.  \n",
    "그러나 강화학습에서는 시행착오를 통해 학습한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "행동을 시뮬레이션해보고, 결과들을 비교하며 어떤 행동이 만족스러운 결과를 가져다 주는 지를 알아낸다.  \n",
    "장기적으로 최대의 보상을 주는 행동을 취하는 정책을 학습한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지도학습의 경우처럼 미리 준비된 유한한 크기의 훈련 데이터셋이 없다.  \n",
    "또다른 강화학습 알고리즘은 주어진 환경에 대해서 행동을 취하도록 안내할 최적의 정책을  \n",
    "각기 다른 기법으로 학습하고 훈련한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "강화학습의 핵심은, 알고리즘이 학습할 정해진 데이터가 없다는 것이다.  \n",
    "그 대신, 환경과 상호작용하고, 피드백을 기반을로 어떤 행동을 취할 것인지 학습하는 에이전트를 구축한다.  \n",
    "그것은 인간처러 관측하고 의사 결정을 내릴 수 있는 시스템을 구축하는 것이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "강화학습의 에이전트에게는 학습할 환경이 주어진다.  에이전트는 환경에 대해 행동을 취하며  \n",
    "이것은 환경의 상태를 바꾸고 양의 보상 또는 음의 보상을 낳는다.  \n",
    "학습 과정에서 취하는 행동들은 무작위적일 수 있지만, 행동에 대한 장기적인 보상을 고려함으로써  \n",
    "더 효과적으로 학습할 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "강화 학습에는 모델-기반 학습이 있고, model-free 학습이 있다.  \n",
    "이에 대해 살펴보자. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 기반 강화학습 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "에이전트가 통제할 환경에 대한 모델을 직접 구축하거나 모델이 주어진다.  \n",
    "상태가 S인 환경에서 보상 R을 받기 위해 행동 A를 취할 떄 얻어지는 결과를 얻을 수 있게 해준다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서 모델이라는 용어는 머신러닝 모델보다는 환경에 대해 설명된다.  \n",
    "이제 계획 알고리즘(planning algorithm)을 사용해 임의의 상태에서 최대의 보상을 얻기 위한 최적의 행동을 찾을 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기본적으로 각 상태에 대한 행동들의 여러 조합을 시도하고 모델을 사용해 다음 상태와 보상을 계산하고  \n",
    "최적의 보상 정책을 찾을 수 있다.  \n",
    "이것은 순수한 최적화 문제로 축약된다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그러나 실제 세계에서 환경에 대한 정확한 모델을 만드는 것은 매우 어렵다.  \n",
    "대상 시스템 내부의 물리적 특성을 고려해야 한다. 그리고 고려해야 할 노이즈가 매우 많다.  \n",
    "상태 전이와 여러 다른 행동에 대한 보상을 포함해 모든 상태들을 담아낼 수 있는 시스템 모델 구축하는 것은 매우 비효율적  \n",
    "이들 기법은 제한적이며 고도로 단순화된 시스템에 대하여 유용하다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 기반 강화학습은 환경 내부의 동적 특성을 알고 있는 경우에 사용된다.  \n",
    "행동 A에 영향을 주는 환경 E에 대한 충분한 지식이 있으면 해결이 간단하다는 것이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그러나 실제로는 그렇게 단순하지 않다. 고려해야 할 많은 변수 및 제약 조건들과 환경에 영향을 주는 요인들이 있다.  \n",
    "환경에 대한 정확한 모델을 만드는 것은 대단히 어려운 일이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "자동차 운전의 예 )  \n",
    "스로틀 위치와 브레이킹을 조절해 A에서 B까지 운전해 갈 수 있는 에이전트를 원한다고 하자.  \n",
    "실제 자동차의 운동 특성과 엔진, 브레이크, 스로틀 등 같은 부속품을 고려하고, 바람에 의한 저항과 지면과의 마찰도 생각해야 한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "보행자와 다른 차량들을 찾아서 회피할 수 있는 안전 장치들도 염두해 두어야 한다.  \n",
    "이렇게 복잡한 환경을 정확히 모델링하는 것은 거의 불가능하다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "따라서 결정론적 모델보다는 다른 방법으로 에이전트를 구축해야 한다.  \n",
    "여기서 model-free 에이전트가 필요해진다.  \n",
    "실제로도 model-free 에이전트는 실제 응용에 있어서 널리 사용되는 강화학습 모델이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 프리 강화학습 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "환경에 대한 모델이 없다.  \n",
    "환경의 움직임에 대한 패턴을 판단하기 위해 시행착오에 의한 접근법을 취한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실제 시스템 또는 시뮬레이터에서 시험하고 관측된 결과들로부터 학습한다.  \n",
    "시행착오를 통해 에이전트는 특정 상태들에 대한 보상을 최대화하는 행동들의 패턴을 학습한다.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정책이란 시행자의 행동을 이끄는, 우리가 알고 싶어하는 그 무엇이며, 접근법을 통해  \n",
    "에이전트는 좋은 행동을 하는 정책을 학습한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SARSA, Q러닝, 딥 Q 네트워크와 같은 알고리즘들은  \n",
    "데이터를 분석하고 좋은 정책을 학습하기 위해 다른 접근법을 취한다.  \n",
    "알고리즘이 학습하는 방법에 영향을 주는 핵심은 탐색(exploration)과 이용(exploitation) 간의 절충하는 것이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이용 : \n",
    "- 현재 알려진 양의 보상(또는 강화)에 초점을 맞추고 같은 정책에 따라 행동을 계속하는 것을 의미 \n",
    "- 양의 보상을 받던 행동을 강화하다 음의 보상을 받으면, 그것을 정책으로 학습하고 그 지점에서 멈춘다.  \n",
    "- 어떤 문제에서는 특히 좋은 해에 빠르게 도달할 수 있는 경우는 이용이 좋은 전략이 될 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "탐색 : \n",
    "- 현재 정책에서 벗어나 새로운 것을 시도하는 것 \n",
    "- 음의 행동을 받았던 정도에서 더 적게 행동함으로써, 이번에는 양의 보상을 다시 받게 되면  \n",
    "- 탐색 접근법이 통한 것이다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "강화학습 알고리즘은 상태를 기반으로 에이전트를 위한 올바른 행동을 결정하기 위해 여러 가지 정책들을 사용할 수 있다.  \n",
    "예) 랜덤 정책 :  \n",
    "- 에이전트가 랜덤 행동을 취하게 할 것\n",
    "- 임계값을 학습할 때까지 계속해서 랜덤으로 수행하고 양/음의 보상을 받는 행동을 계속함 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "탐욕적 정책 :  \n",
    "- 계속 행동을 반복하고 가장 즉각적인 보상을 주는 행동을 선택하는 정책 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q-러닝 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "장기적 보상을 최대화하는 정책을 선택하는 것  \n",
    "환경이 특정 상태일 때 특정 행동을 취함으로써 획득되는 장기적 보상을 측정하는 Q-값을 이용하는 것  \n",
    "Q-러닝 테이블 및 Q-테이블은 가능한 상태들의 수만큼 행을 가지며  \n",
    "가능한 모든 행동들의 수만큼의 열을 갖는다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q 테이블의 초깃값은 모든 값이 0이다.  \n",
    "관련되지 않은 상태 및 행동들에 대한 cell들은 계속 0이 된다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "훈련 또는 학습 과정을 수행하면서, 처음부터 끝까지 각 시도를 수행해 각기 수집된 보상들을 찾는다.  \n",
    "각 시도에서, 각 상태 행동 조합에 대해 벨만 방정식을 사용해 Q값을 계산한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Q_new(st, at) = (1-k)*Q(st, at) + k*(rt + gamma * maxQ(st+1, a))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "행동을 하지 않았을 때의 기대 이득과,  \n",
    "행동을 했을 때의 a의 확률에 대한 가장 높은 최적 미래 값의 추정치를 더한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실제 문제에서는 고려해야 할 변수와 상태-행동 조합이 너무나 많다.  \n",
    "가능한 모든 조합을 보는 것이 거의 불ㄹ가능하다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q 러닝은 매우 효과적이기는 하지만 커다란 제약이 있다.  \n",
    "컴퓨터 메모리에 전부 들어가는 유한 크기의 테이블로 구축할 수 있는 유한 상태의 집합에 대해서는 잘 수행한다.  \n",
    "그러나 문제가 더 복잡해지고 상태 수가 수백 개에서 수백만 개로 커지면 효과적이지 않게 된다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "특정 상태에 대한 Q테이블 값이 없으면 에이전트는 무슨 행동을 취해야 할 지 알 수 없게 된다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이러한 문제를 해결학기 위해 새롭게 고안된 방식이 바로 딥-큐 네트워크이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 딥 큐 네트워크(DQN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "주어진 상태-행동 조합에 대해서 Q값을 예측하는 방법이 필요하다.  \n",
    "그것이 딥 큐 네트워크라고 하는 신경망이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DQN은 상태-행동 쌍의 여러 다른 조합에 대해 신경망을 훈련시키며 종속변수로써 Q값을 구축한다.  \n",
    "DQN은 알려진 상태들에 대한 Q값을 예측하고 최선의 행동을 선택할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "체스보드와 같은 입력 매체의 이미지를 제공하고 이것을 이용해 상태를 디코딩하는 기법이 널리 사용되고 있다.  \n",
    "신경망은 우선 픽셀 값들의 배열인 이미지로부터 상태를 디코딩한다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그런 다음 디코딩된 상태가 Q값을 예측하는 방법을 학습하는 데 사용된다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
